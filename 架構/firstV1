YOLO

YOLO為You Only Look Once的縮寫，指的是只需要對圖片做一次卷積後便能夠判斷出圖形內的物體位置與類別。此模型可實時辨識影像中的各種物體，首先會將影像分成多個網格單位，並在每個網格單元進行多個邊界框的預測和屬於哪個類別的機率預測，最後用閥值和非極大值抑制演算法(Non-Maximum Suppression, NMS)的方式得到結果。YOLO可應用到物件偵測、影像分割、姿態估計、動作辨識等領域。總結來說，YOLO模型以其高效率和多功能性在影像辨識領域中嶄露頭角，讓影像辨識技術得以更精確和高效，開啟了許多新的應用可能性。
過去學者也在眾多的領域中使用了YOLO模型，像是:[ https://www.sciencedirect.com/science/article/pii/S0926580524002723]使用了增強後的YOLOv7版本來偵測出地板缺漏處的洞口並在周圍生成出虛擬圍籬，當建築工人在高處作業時，若靠近電子圍籬則能夠及時發現危險，達到事故預防的目的。[https://www.sciencedirect.com/science/article/pii/S026322412401399X]應用了YOLOv8-Pose來定位每個作業人員與生成骨骼關節的姿勢估計模型，並結合人物重識別(Person Re-Identification, ReID)模型，在不同的動態場景下來準確追蹤多個作業工人。[https://www.sciencedirect.com/science/article/pii/S1361841524001336]提出了 YOLO-infantPose模型與STAPose3D模型，分別用於偵測2D的嬰兒姿態估計與3D的嬰兒姿態估計。[https://www.sciencedirect.com/science/article/pii/S0278612522000243]提出了使用YOLOv3結合VGG16的動作辨識架構，先偵測每個操作員的工作區域，找到該作業員的工作區域並建立相對應的動作資料集，在應用VGG16模型預測每一幀的動作類別，實現動作監控與自動分析的目的，降低傳統IE時間分析所需的人力成本。
該模型為近年來討論度極高且陸續推出準確度更高的新版本，對於姿態估計與動作偵測皆被許多學者所使用。而本研究也為了降低人員作業時發生危害風險，將針對作業人員之動作安全來進行偵測。既能實時監控人員安全，還可以減少人力成本來進行監控。


